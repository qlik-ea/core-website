{
    "docs": [
        {
            "location": "/", 
            "text": "Home\n\n\n\nThis is the top-level repo containing general information, concepts, specifications, examples, and links to other useful resources.\n\n\nThis is the top-level repo containing general information, concepts, specifications, examples, and links to other useful resources.\n\n\nTerminology\n\n\nTerminology used can be found in \nTerminology\n.\n\n\nDeliverables\n\n\nFrontira consists of different deliverables:\n\n\n\n\nUse cases\n\n\nRecipes\n\n\nCore components\n\n\nCustom Qliktive components\n\n\n\n\nRecipes\n\n\nA recipe typically answers a question of the form \"\nHow do I ...?\n\". It is a way to describe important concepts in more isolation.\n\n\nOne example can be: \nHow do I load my own data into QIX Engine?\n\n\nMore information on recipes is provided in the \nRecipes\n overview.\n\n\nServices\n\n\nThis is an overview of the services provided in Frontira. Further information on each service can be found by following the provided links.\n\n\n\n\n\n\n\n\nService\n\n\nFeature\n\n\nDocker Image\n\n\nSource Code\n\n\n\n\n\n\n\n\n\n\nQIX Engine\n\n\nThe powerful associative indexing engine from Qlik and the foundation of Frontira\n\n\nqlikea/engine\n\n\nClosed source\n\n\n\n\n\n\nLicense Service\n\n\nService required to run QIX Engine with a valid license\n\n\nqlikea/license-service\n\n\nClosed source\n\n\n\n\n\n\nMira\n\n\nQIX Engine discovery service\n\n\nqlikea/mira\n\n\nGitHub\n\n\n\n\n\n\n\n\nAll services follow a \ncontract\n.\n\n\nLibraries\n\n\nThere are also several open source libraries that are useful when working with Frontira:\n\n\n\n\n\n\n\n\nLibrary\n\n\nFeature\n\n\nSource Code\n\n\n\n\n\n\n\n\n\n\nenigma.js\n\n\nCommunication with the QIX Engine\n\n\nGitHub\n\n\n\n\n\n\nhalyard.js\n\n\nSimplifies data loading into the QIX Engine\n\n\nGitHub\n\n\n\n\n\n\nafter-work.js\n\n\nUnified testing framework for different test levels\n\n\nGitHub\n\n\n\n\n\n\npicasso.js\n\n\nVisualization library on top of the QIX Engine\n\n\nNot released yet\n\n\n\n\n\n\n\n\nCustom Qliktive Components\n\n\nThese components are packages and microservices that are developed specifically for a certain Qliktive use case. As such, they are not generally re-usable between different solutions. The idea is rather to provide examples and with some parts that can be further built upon for any new solution. Each such component is developed in a separate repo.\n\n\nSince these components are more of example implementations, they do not come with the same level of support from Qlik as the core Components described above.\n\n\nWe use the prefix \nqliktive-\n to make it clear that these components are specific for Qliktive use cases.", 
            "title": "Home"
        }, 
        {
            "location": "/#home", 
            "text": "This is the top-level repo containing general information, concepts, specifications, examples, and links to other useful resources.  This is the top-level repo containing general information, concepts, specifications, examples, and links to other useful resources.", 
            "title": "Home"
        }, 
        {
            "location": "/#terminology", 
            "text": "Terminology used can be found in  Terminology .", 
            "title": "Terminology"
        }, 
        {
            "location": "/#deliverables", 
            "text": "Frontira consists of different deliverables:   Use cases  Recipes  Core components  Custom Qliktive components", 
            "title": "Deliverables"
        }, 
        {
            "location": "/#recipes", 
            "text": "A recipe typically answers a question of the form \" How do I ...? \". It is a way to describe important concepts in more isolation.  One example can be:  How do I load my own data into QIX Engine?  More information on recipes is provided in the  Recipes  overview.", 
            "title": "Recipes"
        }, 
        {
            "location": "/#services", 
            "text": "This is an overview of the services provided in Frontira. Further information on each service can be found by following the provided links.     Service  Feature  Docker Image  Source Code      QIX Engine  The powerful associative indexing engine from Qlik and the foundation of Frontira  qlikea/engine  Closed source    License Service  Service required to run QIX Engine with a valid license  qlikea/license-service  Closed source    Mira  QIX Engine discovery service  qlikea/mira  GitHub     All services follow a  contract .", 
            "title": "Services"
        }, 
        {
            "location": "/#libraries", 
            "text": "There are also several open source libraries that are useful when working with Frontira:     Library  Feature  Source Code      enigma.js  Communication with the QIX Engine  GitHub    halyard.js  Simplifies data loading into the QIX Engine  GitHub    after-work.js  Unified testing framework for different test levels  GitHub    picasso.js  Visualization library on top of the QIX Engine  Not released yet", 
            "title": "Libraries"
        }, 
        {
            "location": "/#custom-qliktive-components", 
            "text": "These components are packages and microservices that are developed specifically for a certain Qliktive use case. As such, they are not generally re-usable between different solutions. The idea is rather to provide examples and with some parts that can be further built upon for any new solution. Each such component is developed in a separate repo.  Since these components are more of example implementations, they do not come with the same level of support from Qlik as the core Components described above.  We use the prefix  qliktive-  to make it clear that these components are specific for Qliktive use cases.", 
            "title": "Custom Qliktive Components"
        }, 
        {
            "location": "/getting-started/prerequisites/", 
            "text": "Suggested", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/getting-started/download/", 
            "text": "Suggested", 
            "title": "Download"
        }, 
        {
            "location": "/documentation/services/qix-engine/", 
            "text": "TODO - Frontira QIX Engine documentation\n\n\nProbably basic information in Frontira context and then providing links to other resources.", 
            "title": "QIX Engine"
        }, 
        {
            "location": "/documentation/services/qix-engine/#todo-frontira-qix-engine-documentation", 
            "text": "Probably basic information in Frontira context and then providing links to other resources.", 
            "title": "TODO - Frontira QIX Engine documentation"
        }, 
        {
            "location": "/documentation/services/license-service/", 
            "text": "License Service\n\n\nThe \nLicense Service\n is required to run QIX Engine with a valid license.\n\n\nThe License Service shall be configured and deployed together with the QIX Engine. In a deployment, each QIX Engine unlocks itself by checking for a valid license, both at start up and periodically, using the License Service.\n\n\nAs such, the License Service API is not primarily intended to be consumed by applications built on top of Frontira. The License Service only works as a required side-car service to QIX Engine.\n\n\nDistribution\n\n\nThe License Service is distributed as a Docker image and is available on Docker Hub as \nqlikea/license-service\n.\n\n\nThe License Service is developed by Qlik as closed source.\n\n\nConfiguration\n\n\nLicense Service\n\n\nThe License Service shall be configured with information on the license to use with the deployment of QIX Engine. This is done by providing two environment variables to the License Service:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLEF_SERIAL\n\n\nThe LEF serial number which identifies the license to use.\n\n\n\n\n\n\nLEF_CONTROL\n\n\nA control number used to validate the LEF serial number.\n\n\n\n\n\n\n\n\nIf these environment variables are not provided, or contain incorrect values, the License Service will still start, but it will deny all activation request with error status codes.\n\n\nThe information can also be provided in files by instead using the envionment variables\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nLEF_SERIAL_FILE\n\n\nPath to file containing the LEF serial number.\n\n\n\n\n\n\nLEF_CONTROL_FILE\n\n\nPath to file containing the control number used to validate the LEF serial number.\n\n\n\n\n\n\n\n\nThis can be useful, for example when providing the values using Docker secrets in Docker Swarm.\n\n\nQIX Engine\n\n\nThe QIX Engine must also be configured with the URL of the License Service in order to make license activation requests to it. this is done by providing the following command line argument to QIX Engine:\n\n\n-S LicenseServiceURL=\nLicense Service URL\n\n\n\n\n\nExamples\n\n\nFor running examples of basic license configuration, see the \nCore Recipe\n.\n\n\nDeployment\n\n\nSingle Instance\n\n\nNormally, only one License Service instance should be required in a Frontira deployment. Traffic between QIX Engine instances and the License Service is low. For load balancing or availability concerns, running multiple replicas of the License Service is supported.\n\n\nLEF back-end connectivity\n\n\nWhen deployed, the License Service must have access to the Qlik licensing back-end, which means it needs to be able to access the following external URLs:\n\n\n\n\nhttp://lef1.qliktech.com\n\n\nhttp://lef2.qliktech.com\n\n\nhttp://lef3.qliktech.com\n\n\n\n\nThe hosting environment may need to open access to these locations, otherwise activation requests will fail.\n\n\nMonitoring\n\n\nHealth Check\n\n\nThe License Service exposes \n/v1/health\n on port 9200, for health checking. Always responds with \n200 OK\n.\n\n\nMetrics\n\n\nThe License Service exposes \n/v1/metrics\n on port 9200, for Prometheus metrics scraping. The following metrics are provided.\n\n\n\n\n\n\n\n\nName\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhttp_requests_total\n\n\nCOUNTER\n\n\nTotal number of HTTP requests since service start.\n- Includes both successful and unsuccessful requests\n- HTTP method and endpoint path are provided as labels\n\n\n\n\n\n\nhttp_request_failures_total\n\n\nCOUNTER\n\n\nTotal number of HTTP request failures since service start.\n- HTTP method and endpoint path are provided as labels\n\n\n\n\n\n\nhttp_request_duration_seconds\n\n\nHISTOGRAM\n\n\nDurations of HTTP request since service start.\n- HTTP method and endpoint path are provided as labels.\n\n\n\n\n\n\nactive_instances\n\n\nGAUGE\n\n\nNumber of active instances (Engines with an active license).\n\n\n\n\n\n\nlicense_expiry_seconds\n\n\nGAUGE\n\n\nNumber of seconds until the license expires.\n\n\n\n\n\n\n\n\nLogging\n\n\nThe License Service complies with logging as described in the Frontira \nContract\n. By default, the minimum log level is \ninfo\n. However, for the Microsoft libraries the default minimum logging level is set to \nwarning\n to avoid too verbose logging.\n\n\nThe default minimum logging level can be overridden by providing the \nLOG_LEVEL\n environment variable.\n\n\nIf provided, this will affect the logging level for all components in the License Service, including Microsoft libraries. It is recommended to use the default minimum level in a production environment, and only override the level for debugging purposes.", 
            "title": "License Service"
        }, 
        {
            "location": "/documentation/services/license-service/#license-service", 
            "text": "The  License Service  is required to run QIX Engine with a valid license.  The License Service shall be configured and deployed together with the QIX Engine. In a deployment, each QIX Engine unlocks itself by checking for a valid license, both at start up and periodically, using the License Service.  As such, the License Service API is not primarily intended to be consumed by applications built on top of Frontira. The License Service only works as a required side-car service to QIX Engine.", 
            "title": "License Service"
        }, 
        {
            "location": "/documentation/services/license-service/#distribution", 
            "text": "The License Service is distributed as a Docker image and is available on Docker Hub as  qlikea/license-service .  The License Service is developed by Qlik as closed source.", 
            "title": "Distribution"
        }, 
        {
            "location": "/documentation/services/license-service/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/documentation/services/license-service/#license-service_1", 
            "text": "The License Service shall be configured with information on the license to use with the deployment of QIX Engine. This is done by providing two environment variables to the License Service:     Name  Description      LEF_SERIAL  The LEF serial number which identifies the license to use.    LEF_CONTROL  A control number used to validate the LEF serial number.     If these environment variables are not provided, or contain incorrect values, the License Service will still start, but it will deny all activation request with error status codes.  The information can also be provided in files by instead using the envionment variables     Name  Description      LEF_SERIAL_FILE  Path to file containing the LEF serial number.    LEF_CONTROL_FILE  Path to file containing the control number used to validate the LEF serial number.     This can be useful, for example when providing the values using Docker secrets in Docker Swarm.", 
            "title": "License Service"
        }, 
        {
            "location": "/documentation/services/license-service/#qix-engine", 
            "text": "The QIX Engine must also be configured with the URL of the License Service in order to make license activation requests to it. this is done by providing the following command line argument to QIX Engine:  -S LicenseServiceURL= License Service URL", 
            "title": "QIX Engine"
        }, 
        {
            "location": "/documentation/services/license-service/#examples", 
            "text": "For running examples of basic license configuration, see the  Core Recipe .", 
            "title": "Examples"
        }, 
        {
            "location": "/documentation/services/license-service/#deployment", 
            "text": "", 
            "title": "Deployment"
        }, 
        {
            "location": "/documentation/services/license-service/#single-instance", 
            "text": "Normally, only one License Service instance should be required in a Frontira deployment. Traffic between QIX Engine instances and the License Service is low. For load balancing or availability concerns, running multiple replicas of the License Service is supported.", 
            "title": "Single Instance"
        }, 
        {
            "location": "/documentation/services/license-service/#lef-back-end-connectivity", 
            "text": "When deployed, the License Service must have access to the Qlik licensing back-end, which means it needs to be able to access the following external URLs:   http://lef1.qliktech.com  http://lef2.qliktech.com  http://lef3.qliktech.com   The hosting environment may need to open access to these locations, otherwise activation requests will fail.", 
            "title": "LEF back-end connectivity"
        }, 
        {
            "location": "/documentation/services/license-service/#monitoring", 
            "text": "", 
            "title": "Monitoring"
        }, 
        {
            "location": "/documentation/services/license-service/#health-check", 
            "text": "The License Service exposes  /v1/health  on port 9200, for health checking. Always responds with  200 OK .", 
            "title": "Health Check"
        }, 
        {
            "location": "/documentation/services/license-service/#metrics", 
            "text": "The License Service exposes  /v1/metrics  on port 9200, for Prometheus metrics scraping. The following metrics are provided.     Name  Type  Description      http_requests_total  COUNTER  Total number of HTTP requests since service start. - Includes both successful and unsuccessful requests - HTTP method and endpoint path are provided as labels    http_request_failures_total  COUNTER  Total number of HTTP request failures since service start. - HTTP method and endpoint path are provided as labels    http_request_duration_seconds  HISTOGRAM  Durations of HTTP request since service start. - HTTP method and endpoint path are provided as labels.    active_instances  GAUGE  Number of active instances (Engines with an active license).    license_expiry_seconds  GAUGE  Number of seconds until the license expires.", 
            "title": "Metrics"
        }, 
        {
            "location": "/documentation/services/license-service/#logging", 
            "text": "The License Service complies with logging as described in the Frontira  Contract . By default, the minimum log level is  info . However, for the Microsoft libraries the default minimum logging level is set to  warning  to avoid too verbose logging.  The default minimum logging level can be overridden by providing the  LOG_LEVEL  environment variable.  If provided, this will affect the logging level for all components in the License Service, including Microsoft libraries. It is recommended to use the default minimum level in a production environment, and only override the level for debugging purposes.", 
            "title": "Logging"
        }, 
        {
            "location": "/documentation/services/mira/", 
            "text": "Mira\n\n\nMira\n provides QIX Engine instance discovery in containerized environments.\n\n\nThe purpose of the service is for other services to be able to get a set of available QIX Engine instances with properties of each QIX Engine instance. This information can be used to take decisions on, for example which engine that is suitable to open a new session towards, or if there is a need to start a new QIX Engine instance.\n\n\nDistribution\n\n\nMira is distributed as a Docker image and is available on Docker Hub as \nqlikea/mira\n.\n\n\nMira is available as open source on \nGitHub\n.\n\n\nAPI\n\n\nThe \napi-doc.yml\n provides an OpenAPI specification of the Mira REST API.\n\n\nMira exposes its REST API on port 9100.\n\n\nOperation Modes\n\n\nMira supports different operation modes. The operation mode determines how Mira discovers QIX Engine instances. The following operation modes are supported:\n\n\n\n\nSwarm\n - Discovers QIX Engine instances in a Docker Swarm environment\n\n\nKubernetes\n - Discovers QIX Engine instances in a Kubernstes environment\n\n\nDNS\n - Discovers QIX Engine instances using DNS service look-ups\n\n\nLocal\n - Discovers QIX Engine instances running on the local Docker Engine, typically created using \ndocker-compose\n on a local machine\n\n\n\n\nThe operation mode is set by providing the environment variable \nMIRA_MODE\n to the Mira container.\n\n\nQIX Engine Labeling\n\n\nIn all modes, except \nDNS\n, Mira uses labels to identify QIX engine instances. By default, the label Mira searches for is \nqix-engine\n but can be configured using the \nMIRA_DISCOVERY_LABEL\n environment variable. Note that Mira only looks at the label key and ignores its value. The values can even be omitted. Each section on the different modes shows examples of discovery labeling.\n\n\nPort Labeling\n\n\nIn all modes, except \nDNS\n, Mira also identifies labels on QIX Engine instances to determine which ports that are used to serve the QIX API (websocket) on, and which port it serves the \n/metrics\n endpoint on. By default, Mira identifies and uses the values of the labels \nqix-engine-api-port\n and \nqix-engine-metrics-port\n. These label keys can be configured using the environment variables \nMIRA_ENGINE_API_PORT_LABEL\n and \nMIRA_ENGINE_METRICS_PORT_LABEL\n respectively.\n\n\nIf a QIX Engine instance does not have port labels set, Mira defaults to setting the QIX API port to 9076 and the \n/metrics\n port to 9090.\n\n\nEnvironment Variables\n\n\nThe following environment variables can optionally be set for Mira:\n\n\n\n\n\n\n\n\nName\n\n\nDefault value\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMIRA_MODE\n\n\nswarm\n\n\nThe operation mode of Mira.\n- Can be \nswarm\n, \nkubernetes\n, \ndns\n, or \nlocal\n.\n\n\n\n\n\n\nMIRA_DISCOVERY_LABEL\n\n\nqix-engine\n\n\nLabel key that Mira uses to identify engine instances.\n- Applicable in modes \nswarm\n, \nkubernetes\n, and \nlocal\n\n\n\n\n\n\nMIRA_DISCOVERY_HOSTNAME\n\n\nn/a\n\n\nHostname that Mira uses to query DNS for QIX Engine instances.\n- Only applicable in mode \ndns\n\n\n\n\n\n\nMIRA_ENGINE_API_PORT_LABEL\n\n\nqix-engine-api-port\n\n\nLabel that Mira will use to determine the QIX API (websocket) port.\n- Applicable in modes \nswarm\n, \nkubernetes\n, and \nlocal\n\n\n\n\n\n\nMIRA_ENGINE_METRICS_PORT_LABEL\n\n\nqix-engine-metrics-port\n\n\nLabel that Mira will use to determine the \n/metrics\n port.\n- Applicable in modes \nswarm\n, \nkubernetes\n, and \nlocal\n\n\n\n\n\n\nMIRA_ENGINE_DISCOVERY_REFRESH_RATE\n\n\n1000\n\n\nRefresh rate in milliseconds for discovering QIX Engine instances.\n\n\n\n\n\n\nMIRA_ENGINE_HEALTH_REFRESH_RATE\n\n\n5000\n\n\nRefresh rate in milliseconds for checking if QIX Engine instances are healthy.\n\n\n\n\n\n\nMIRA_KUBERNETES_PROXY_PORT\n\n\n8001\n\n\nPort that Mira will use to communicate to the Kubernetes API server.\n\n\n\n\n\n\nMIRA_LOG_LEVEL\n\n\ninfo\n\n\nMinimum log level that Mira outputs when logging to \nstdout\n.\n\n\n\n\n\n\n\n\nLogging\n\n\nMira follows the logging format and levels specified in the \ncontract\n.\n\n\nDefault minimum log level is \ninfo\n, but can be set using the \nMIRA_LOG_LEVEL\n environment variable.\n\n\nSwarm Mode\n\n\nIn \nSwarm\n mode, Mira assumes that all QIX Engine instances run as Docker Swarm services inside one single Docker Swarm cluster. \nSwarm\n mode is enabled by setting the environment variable \nMIRA_MODE\n to \nswarm\n before starting the Mira Docker service.\n\n\nMira \nmust\n be configured to run on a Swarm manager node, since it needs to communicate to a manager Docker Engine.\n\n\nExample\n\n\nThe file \ndocker-compose-swarm.yml\n shows an example of how Mira can be started in Swarm mode, together with a QIX Engine instance that is labeled so that Mira will discover it. It assumed that a Docker Swarm cluster is already created with at least one manager, and that the Docker CLI client is configured to issue commands towards the manager node.\n\n\nTo deploy Mira and QIX Engine in a stack named \nmira-stack\n, run:\n\n\n$ docker stack deploy -c docker-compose-swarm.yml mira-stack\n\n\n\n\nTo remove the stack, run:\n\n\n$ docker stack rm mira-stack\n\n\n\n\nLabeling\n\n\nIn \nSwarm\n mode, Mira assumes that labels are provided on Docker containers. Below is an example extract from a Docker stack file that would cause Mira to discover both QIX Engine replicas as two separate instances of the \nqix-engine1\n service.\n\n\nversion: \n3.1\n\n\nservices:\n  mira:\n    image: qlikea/mira\n    environment:\n     - MIRA_MODE=swarm\n    ...\n\n  qix-engine1:\n    image: qlikea/engine\n    labels:\n      qix-engine: \n\n      qix-engine-api-port: \n9076\n\n      qix-engine-metrics-port: \n9090\n\n\n    deploy:\n      replicas: 2\n      placement:\n        ...\n\n\n\n\n\nNote that in Docker Swarm, the labels must be set on container level, \noutside\n the \ndeploy:\n scope. Setting the labels in the \ndeploy:\n scope causes the labels to be set on the service only, and not on each individual container (task) of the service. Only labeling the service will not make Mira discover the engines.\n\n\nLabeling outside the \ndeploy:\n scope also has the benefit of that the labeling scheme for \nLocal\n and \nSwarm\n mode becomes similar.\n\n\nKubernetes Mode\n\n\nIn \nKubernetes\n mode, Mira assumes that all QIX Engine instances run as pods in the Kubernetes cluster. \nKubernetes\n mode is set by setting the \nMIRA_MODE\n environment variable to \nkubernetes\n before starting the Mira pod.\n\n\nSince Mira needs to communicate with the Kubernetes API server, a \nkubectl\n proxy should be set up in the Kubernetes deployment. A convenient way to do this is to bundle the \nkubectl\n proxy as a container in the same pod as the Mira container. In this way, Mira can reach the proxy on \nlocalhost\n.\n\n\nExample\n\n\nIn order to deploy Mira and QIX Engine instances to Kubernetes, it is assumed that a Kubernetes cluster exists and is configured properly. A quick way to do this, for experimental purposes, is to use \nMiniKube\n.\n\n\nTo start Mira in \nKubernetes\n mode, the \nkubectl\n command line tool can be used. Preferably, a Kubernetes deployment YAML file is used; for example\n\n\n$ kubectl apply -f mira-deployment.yml\n\n\n\n\nThe file \nmira-deployment.yml\n shows an example. Note how the deployment also bundles the \nkubectl\n proxy into the same pod. Since Kubernetes must be able to pull Docker images, the deployment file assumes that Kubernetes is configured with Docker Hub registry credentials in a secret named \ndockerhub\n.\n\n\nNormally the Mira REST API shall also be exposed as a service. Preferably, this can also be done by applying the service configuration as a YML file; for example\n\n\n$ kubectl apply -f mira-service.yml\n\n\n\n\nThe file \nmira-service.yml\n show an example of this where Mira's default port 9100 is exposed outside the cluster as port 31000 (using the \nNodePort\n type). Assuming \nminikube\n is used to create the cluster, the Mira health check should now be possible to reach.\n\n\n$ curl http://$(minikube ip):31000/v1/health\n\n\n\n\nIn order for Mira to discover QIX Engine instances in the cluster, a Kubernetes deployment file can also be used.\n\n\n$ kubectl apply -f engine-deployment.yml\n\n\n\n\nThe file \nengine-deployment.yml\n shows an example of a deployment of two engine pod replicas. However, this is not enough for Mira to be able to discover the two engine instances. For this to happen, the engines need to be exposed as services with named ports. For example\n\n\n$ kubectl apply -f engine-service.yml\n\n\n\n\nThe file \nengine-service.yml\n show as example of how the engine pods are exposed as a service with a named port, \nqix\n. Each engine replica will appear in the \nendpoints\n object that will be related to the service and Mira uses this information to list the engine instances. This list should now be possible to retrieve with\n\n\n$ curl http://$(minikube ip):31000/v1/engines\n\n\n\n\nNote that the example files here only provide a minimal setup in order to get Mira up and running with Kubernetes. In a production deployment, many other aspects must be considered.\n\n\nLabeling\n\n\nIn \nKubernetes\n mode, Mira assumes that the discovery label is provided on pods hosting Engine containers. Below is an example extract from a Kubernetes deployment file for two Engine replicas where the label is set up so that Mira can discover them both.\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: engine-deployment\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        qix-engine: \n\n        qix-engine-api-port: \n9076\n\n        qix-engine-metrics-port: \n9090\n\n    spec:\n      containers:\n        ...\n        image: qlikea/engine\n        ...      \n\n\n\n\nNOTE\n - Mira does not support hosting multiple engine containers inside the same pod, since they would get the same IP address and port.\n\n\nDNS mode\n\n\nIn \nDNS\n mode Mira resolves by hostname, and uses all returned IP addresses it finds to fetch additional QIX Engine instance data. \nDNS\n mode is enabled by setting the environment variable \nMIRA_MODE\n to \ndns\n.\n\n\nHostname Configuration\n\n\nIn the \nDNS\n mode there is no need to set labels on QIX Engine instances. Instead, the hostname used to resolve QIX Engine instances must be set using the \nMIRA_DISCOVERY_HOSTNAME\n environment variable.\n\n\nExample\n\n\nAn example how this could be configured in a Docker Swarm environment can be found in the file \ndocker-compose-dns.yml\n.\n\n\nNote the environment variables used to set the \nDNS\n mode and the hostname used to identify QIX Engine instances, which correspond to the service name \nqix-engine\n given and how Docker Swarm, by default, assigns DNS names to services.\n\n\nservices:\n  mira:\n    ...\n    environment:\n     - MIRA_MODE=dns\n     - MIRA_DISCOVERY_HOSTNAME=tasks.qix-engine\n    ...\n\n  qix-engine:\n    ...\n\n\n\n\nLocal Mode\n\n\nIn \nLocal\n mode, Mira assumes that all QIX Engine instances run as Docker containers on the \nlocalhost\n Docker Engine, without any orchestration platform such as Docker Swarm or Kubernetes. \nLocal\n mode is enabled by setting the \nMIRA_MODE\n environment variable to \nlocal\n when starting the Mira Docker container.\n\n\nExample\n\n\nTypically, a set of Docker container are started locally using a Docker compose. The file \ndocker-compose.yml\n shows an example of this. It starts one Mira container and two QIX Engine containers.\n\n\nAssuming the current \ndocker-compose.yml\n exists in the current working directory, run:\n\n\n$ docker-compose up -d\n\n\n\n\nTo verify that Mira discovers the two QIX Engine containers, run:\n\n\n$ curl http://localhost:9100/v1/engines\n\n\n\n\nLabeling\n\n\nIn \nLocal\n mode, Mira assumes that labels are provided on Docker containers. Below is an example extract from a Docker compose file with labels present. Since \nqix-engine-api-port\n and \nqix-engine-metrics-port\n are optional and with the same default values, that could have been omitted.\n\n\nversion: \n3.1\n\n\nservices:\n  mira:\n    image: qlikea/mira\n    ...\n\n  engine1:\n    image: qlikea/engine\n    ...\n    labels:\n      qix-engine: \n\n      qix-engine-api-port: \n9076\n\n      qix-engine-metrics-port: \n9090", 
            "title": "Mira"
        }, 
        {
            "location": "/documentation/services/mira/#mira", 
            "text": "Mira  provides QIX Engine instance discovery in containerized environments.  The purpose of the service is for other services to be able to get a set of available QIX Engine instances with properties of each QIX Engine instance. This information can be used to take decisions on, for example which engine that is suitable to open a new session towards, or if there is a need to start a new QIX Engine instance.", 
            "title": "Mira"
        }, 
        {
            "location": "/documentation/services/mira/#distribution", 
            "text": "Mira is distributed as a Docker image and is available on Docker Hub as  qlikea/mira .  Mira is available as open source on  GitHub .", 
            "title": "Distribution"
        }, 
        {
            "location": "/documentation/services/mira/#api", 
            "text": "The  api-doc.yml  provides an OpenAPI specification of the Mira REST API.  Mira exposes its REST API on port 9100.", 
            "title": "API"
        }, 
        {
            "location": "/documentation/services/mira/#operation-modes", 
            "text": "Mira supports different operation modes. The operation mode determines how Mira discovers QIX Engine instances. The following operation modes are supported:   Swarm  - Discovers QIX Engine instances in a Docker Swarm environment  Kubernetes  - Discovers QIX Engine instances in a Kubernstes environment  DNS  - Discovers QIX Engine instances using DNS service look-ups  Local  - Discovers QIX Engine instances running on the local Docker Engine, typically created using  docker-compose  on a local machine   The operation mode is set by providing the environment variable  MIRA_MODE  to the Mira container.", 
            "title": "Operation Modes"
        }, 
        {
            "location": "/documentation/services/mira/#qix-engine-labeling", 
            "text": "In all modes, except  DNS , Mira uses labels to identify QIX engine instances. By default, the label Mira searches for is  qix-engine  but can be configured using the  MIRA_DISCOVERY_LABEL  environment variable. Note that Mira only looks at the label key and ignores its value. The values can even be omitted. Each section on the different modes shows examples of discovery labeling.", 
            "title": "QIX Engine Labeling"
        }, 
        {
            "location": "/documentation/services/mira/#port-labeling", 
            "text": "In all modes, except  DNS , Mira also identifies labels on QIX Engine instances to determine which ports that are used to serve the QIX API (websocket) on, and which port it serves the  /metrics  endpoint on. By default, Mira identifies and uses the values of the labels  qix-engine-api-port  and  qix-engine-metrics-port . These label keys can be configured using the environment variables  MIRA_ENGINE_API_PORT_LABEL  and  MIRA_ENGINE_METRICS_PORT_LABEL  respectively.  If a QIX Engine instance does not have port labels set, Mira defaults to setting the QIX API port to 9076 and the  /metrics  port to 9090.", 
            "title": "Port Labeling"
        }, 
        {
            "location": "/documentation/services/mira/#environment-variables", 
            "text": "The following environment variables can optionally be set for Mira:     Name  Default value  Description      MIRA_MODE  swarm  The operation mode of Mira. - Can be  swarm ,  kubernetes ,  dns , or  local .    MIRA_DISCOVERY_LABEL  qix-engine  Label key that Mira uses to identify engine instances. - Applicable in modes  swarm ,  kubernetes , and  local    MIRA_DISCOVERY_HOSTNAME  n/a  Hostname that Mira uses to query DNS for QIX Engine instances. - Only applicable in mode  dns    MIRA_ENGINE_API_PORT_LABEL  qix-engine-api-port  Label that Mira will use to determine the QIX API (websocket) port. - Applicable in modes  swarm ,  kubernetes , and  local    MIRA_ENGINE_METRICS_PORT_LABEL  qix-engine-metrics-port  Label that Mira will use to determine the  /metrics  port. - Applicable in modes  swarm ,  kubernetes , and  local    MIRA_ENGINE_DISCOVERY_REFRESH_RATE  1000  Refresh rate in milliseconds for discovering QIX Engine instances.    MIRA_ENGINE_HEALTH_REFRESH_RATE  5000  Refresh rate in milliseconds for checking if QIX Engine instances are healthy.    MIRA_KUBERNETES_PROXY_PORT  8001  Port that Mira will use to communicate to the Kubernetes API server.    MIRA_LOG_LEVEL  info  Minimum log level that Mira outputs when logging to  stdout .", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/documentation/services/mira/#logging", 
            "text": "Mira follows the logging format and levels specified in the  contract .  Default minimum log level is  info , but can be set using the  MIRA_LOG_LEVEL  environment variable.", 
            "title": "Logging"
        }, 
        {
            "location": "/documentation/services/mira/#swarm-mode", 
            "text": "In  Swarm  mode, Mira assumes that all QIX Engine instances run as Docker Swarm services inside one single Docker Swarm cluster.  Swarm  mode is enabled by setting the environment variable  MIRA_MODE  to  swarm  before starting the Mira Docker service.  Mira  must  be configured to run on a Swarm manager node, since it needs to communicate to a manager Docker Engine.", 
            "title": "Swarm Mode"
        }, 
        {
            "location": "/documentation/services/mira/#example", 
            "text": "The file  docker-compose-swarm.yml  shows an example of how Mira can be started in Swarm mode, together with a QIX Engine instance that is labeled so that Mira will discover it. It assumed that a Docker Swarm cluster is already created with at least one manager, and that the Docker CLI client is configured to issue commands towards the manager node.  To deploy Mira and QIX Engine in a stack named  mira-stack , run:  $ docker stack deploy -c docker-compose-swarm.yml mira-stack  To remove the stack, run:  $ docker stack rm mira-stack", 
            "title": "Example"
        }, 
        {
            "location": "/documentation/services/mira/#labeling", 
            "text": "In  Swarm  mode, Mira assumes that labels are provided on Docker containers. Below is an example extract from a Docker stack file that would cause Mira to discover both QIX Engine replicas as two separate instances of the  qix-engine1  service.  version:  3.1 \n\nservices:\n  mira:\n    image: qlikea/mira\n    environment:\n     - MIRA_MODE=swarm\n    ...\n\n  qix-engine1:\n    image: qlikea/engine\n    labels:\n      qix-engine:  \n      qix-engine-api-port:  9076 \n      qix-engine-metrics-port:  9090 \n\n    deploy:\n      replicas: 2\n      placement:\n        ...  Note that in Docker Swarm, the labels must be set on container level,  outside  the  deploy:  scope. Setting the labels in the  deploy:  scope causes the labels to be set on the service only, and not on each individual container (task) of the service. Only labeling the service will not make Mira discover the engines.  Labeling outside the  deploy:  scope also has the benefit of that the labeling scheme for  Local  and  Swarm  mode becomes similar.", 
            "title": "Labeling"
        }, 
        {
            "location": "/documentation/services/mira/#kubernetes-mode", 
            "text": "In  Kubernetes  mode, Mira assumes that all QIX Engine instances run as pods in the Kubernetes cluster.  Kubernetes  mode is set by setting the  MIRA_MODE  environment variable to  kubernetes  before starting the Mira pod.  Since Mira needs to communicate with the Kubernetes API server, a  kubectl  proxy should be set up in the Kubernetes deployment. A convenient way to do this is to bundle the  kubectl  proxy as a container in the same pod as the Mira container. In this way, Mira can reach the proxy on  localhost .", 
            "title": "Kubernetes Mode"
        }, 
        {
            "location": "/documentation/services/mira/#example_1", 
            "text": "In order to deploy Mira and QIX Engine instances to Kubernetes, it is assumed that a Kubernetes cluster exists and is configured properly. A quick way to do this, for experimental purposes, is to use  MiniKube .  To start Mira in  Kubernetes  mode, the  kubectl  command line tool can be used. Preferably, a Kubernetes deployment YAML file is used; for example  $ kubectl apply -f mira-deployment.yml  The file  mira-deployment.yml  shows an example. Note how the deployment also bundles the  kubectl  proxy into the same pod. Since Kubernetes must be able to pull Docker images, the deployment file assumes that Kubernetes is configured with Docker Hub registry credentials in a secret named  dockerhub .  Normally the Mira REST API shall also be exposed as a service. Preferably, this can also be done by applying the service configuration as a YML file; for example  $ kubectl apply -f mira-service.yml  The file  mira-service.yml  show an example of this where Mira's default port 9100 is exposed outside the cluster as port 31000 (using the  NodePort  type). Assuming  minikube  is used to create the cluster, the Mira health check should now be possible to reach.  $ curl http://$(minikube ip):31000/v1/health  In order for Mira to discover QIX Engine instances in the cluster, a Kubernetes deployment file can also be used.  $ kubectl apply -f engine-deployment.yml  The file  engine-deployment.yml  shows an example of a deployment of two engine pod replicas. However, this is not enough for Mira to be able to discover the two engine instances. For this to happen, the engines need to be exposed as services with named ports. For example  $ kubectl apply -f engine-service.yml  The file  engine-service.yml  show as example of how the engine pods are exposed as a service with a named port,  qix . Each engine replica will appear in the  endpoints  object that will be related to the service and Mira uses this information to list the engine instances. This list should now be possible to retrieve with  $ curl http://$(minikube ip):31000/v1/engines  Note that the example files here only provide a minimal setup in order to get Mira up and running with Kubernetes. In a production deployment, many other aspects must be considered.", 
            "title": "Example"
        }, 
        {
            "location": "/documentation/services/mira/#labeling_1", 
            "text": "In  Kubernetes  mode, Mira assumes that the discovery label is provided on pods hosting Engine containers. Below is an example extract from a Kubernetes deployment file for two Engine replicas where the label is set up so that Mira can discover them both.  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: engine-deployment\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        qix-engine:  \n        qix-engine-api-port:  9076 \n        qix-engine-metrics-port:  9090 \n    spec:\n      containers:\n        ...\n        image: qlikea/engine\n        ...        NOTE  - Mira does not support hosting multiple engine containers inside the same pod, since they would get the same IP address and port.", 
            "title": "Labeling"
        }, 
        {
            "location": "/documentation/services/mira/#dns-mode", 
            "text": "In  DNS  mode Mira resolves by hostname, and uses all returned IP addresses it finds to fetch additional QIX Engine instance data.  DNS  mode is enabled by setting the environment variable  MIRA_MODE  to  dns .", 
            "title": "DNS mode"
        }, 
        {
            "location": "/documentation/services/mira/#hostname-configuration", 
            "text": "In the  DNS  mode there is no need to set labels on QIX Engine instances. Instead, the hostname used to resolve QIX Engine instances must be set using the  MIRA_DISCOVERY_HOSTNAME  environment variable.", 
            "title": "Hostname Configuration"
        }, 
        {
            "location": "/documentation/services/mira/#example_2", 
            "text": "An example how this could be configured in a Docker Swarm environment can be found in the file  docker-compose-dns.yml .  Note the environment variables used to set the  DNS  mode and the hostname used to identify QIX Engine instances, which correspond to the service name  qix-engine  given and how Docker Swarm, by default, assigns DNS names to services.  services:\n  mira:\n    ...\n    environment:\n     - MIRA_MODE=dns\n     - MIRA_DISCOVERY_HOSTNAME=tasks.qix-engine\n    ...\n\n  qix-engine:\n    ...", 
            "title": "Example"
        }, 
        {
            "location": "/documentation/services/mira/#local-mode", 
            "text": "In  Local  mode, Mira assumes that all QIX Engine instances run as Docker containers on the  localhost  Docker Engine, without any orchestration platform such as Docker Swarm or Kubernetes.  Local  mode is enabled by setting the  MIRA_MODE  environment variable to  local  when starting the Mira Docker container.", 
            "title": "Local Mode"
        }, 
        {
            "location": "/documentation/services/mira/#example_3", 
            "text": "Typically, a set of Docker container are started locally using a Docker compose. The file  docker-compose.yml  shows an example of this. It starts one Mira container and two QIX Engine containers.  Assuming the current  docker-compose.yml  exists in the current working directory, run:  $ docker-compose up -d  To verify that Mira discovers the two QIX Engine containers, run:  $ curl http://localhost:9100/v1/engines", 
            "title": "Example"
        }, 
        {
            "location": "/documentation/services/mira/#labeling_2", 
            "text": "In  Local  mode, Mira assumes that labels are provided on Docker containers. Below is an example extract from a Docker compose file with labels present. Since  qix-engine-api-port  and  qix-engine-metrics-port  are optional and with the same default values, that could have been omitted.  version:  3.1 \n\nservices:\n  mira:\n    image: qlikea/mira\n    ...\n\n  engine1:\n    image: qlikea/engine\n    ...\n    labels:\n      qix-engine:  \n      qix-engine-api-port:  9076 \n      qix-engine-metrics-port:  9090", 
            "title": "Labeling"
        }, 
        {
            "location": "/documentation/contract/", 
            "text": "Contract\n\n\nThis document contains a contract for Frontira services to keep the services aligned in aspects such as logging formats, health checks and metrics.\n\n\nLogging\n\n\nServices must log enough information for a developer to be able to debug and fix issues using the information available in the logs. In general, developers will not have access to production environments in order to debug services, but instead access to real-time historical logs for analysis. A service's logs is the main visibility into the behavior of the service. A service should not concern itself with the routing or storage/aggregation of its logs. A service should also not attempt to write to or manage logfiles. Instead a service should write its logs to \nstdout\n and concern itself with making sure that the content of the logs are sufficient for monitoring and fixing issues. In staging and production environments each service's log output will be captured by the execution environment and aggregated in near real-time.\n\n\n\n\nOnly output events on \nstdout\n, never output events to files.\n\n\nEvents \nmust\n be on \nstdout\n and not \nstderr\n.\n\n\nIt is \nnot\n the job of the service to store, transmit or buffer log events.\n\n\nDuring local development, the developer will view the service's event stream in their terminal or console, or manually redirect (with a pipe or similar) to a local file.\n\n\nWhen the service is deployed, it is the responsibility of the execution environment to capture and direct logs appropriately for later analysis.\n\n\n\n\n\n\n\n\nEvents must be outputted as valid JSON, with a minimum of standard fields:\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\nRequirements\n\n\n\n\n\n\n\n\n\n\ntimestamp\n\n\nThe date/time that the event occurred at.\n\n\nThe system clock must be set for the UTC 00:00 timezone (a.k.a \nZulu\n), and events must be logged in this timezone.\nTimestamps must be formatted in the RFC3339 format \nwith fractional seconds\n: \n2006-01-02T15:04:05.999999999Z\nNot all systems will report fractions of a second at the same precision - it doesn't matter whether this is provided a millisecond or nanosecond precision, as long as it's provided.\n\n\n\n\n\n\nmessage\n\n\nA message describing the event.\n\n\nThe message \nshould not\n contain metadata. Any metadata about the event should be presented in seperate fields.\n\n\n\n\n\n\nlogseverity\n\n\nString value indicating the level of importance of the log message.\n\n\nThe \nlogseverity\n specified in the log \nmust\n be one of the following values; \nTRACE\n, \nDEBUG\n, \nINFO\n, \nWARNING\n, \nERROR\n or \nFATAL\n. The definition of each level is further described \nhere\n.\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional fields will be required in specific contexts, and should be coordinated between services so that analysis can be correctly aggregated across different services.\n\n\n\n\nIf multiple services track e.g. session IDs, they should agree on a common field name, such as \nsession_id\n.\n\n\nCase \n word separation style must be consistent. The \nsnake_case\n style should be preferred (e.g. use \nsession_id\n, \nnot\n \nSessionId\n)\n\n\n\n\n\n\n\n\nLogging Levels\n\n\nIn this section each of the allowed log levels are specified and followed with a set of examples where each log level is applicable.\n\n\n\n\nTRACE\n\n\nFine-grained debug message, typically used to capture a flow of events.\n\n\nDEBUG\n\n\nMostly used for debugging purposes.\n\n\nTypically not enabled by default in production.\n\n\nINFO\n\n\nNormal operations of the service that should be logged.\n\n\nIndicate some important operation or state in the service.\n\n\nWARNING\n\n\nIndicate an event or state that was not part of common operations, but was handled.\n\n\nEvent that potentially can become an error.\n\n\nEvent that does not prevent correct operation of the system from an end-user perspective.\n\n\nERROR\n\n\nUnhandled error that was unexpected to the service.\n\n\nService can continue normal operation after error recovery.\n\n\nFATAL\n\n\nNon-recoverable error that forces the service to terminate.\n\n\n\n\nHealth Checks\n\n\ntodo\n\n\nMetrics\n\n\ntodo", 
            "title": "Contract"
        }, 
        {
            "location": "/documentation/contract/#contract", 
            "text": "This document contains a contract for Frontira services to keep the services aligned in aspects such as logging formats, health checks and metrics.", 
            "title": "Contract"
        }, 
        {
            "location": "/documentation/contract/#logging", 
            "text": "Services must log enough information for a developer to be able to debug and fix issues using the information available in the logs. In general, developers will not have access to production environments in order to debug services, but instead access to real-time historical logs for analysis. A service's logs is the main visibility into the behavior of the service. A service should not concern itself with the routing or storage/aggregation of its logs. A service should also not attempt to write to or manage logfiles. Instead a service should write its logs to  stdout  and concern itself with making sure that the content of the logs are sufficient for monitoring and fixing issues. In staging and production environments each service's log output will be captured by the execution environment and aggregated in near real-time.   Only output events on  stdout , never output events to files.  Events  must  be on  stdout  and not  stderr .  It is  not  the job of the service to store, transmit or buffer log events.  During local development, the developer will view the service's event stream in their terminal or console, or manually redirect (with a pipe or similar) to a local file.  When the service is deployed, it is the responsibility of the execution environment to capture and direct logs appropriately for later analysis.     Events must be outputted as valid JSON, with a minimum of standard fields:     Field  Description  Requirements      timestamp  The date/time that the event occurred at.  The system clock must be set for the UTC 00:00 timezone (a.k.a  Zulu ), and events must be logged in this timezone. Timestamps must be formatted in the RFC3339 format  with fractional seconds :  2006-01-02T15:04:05.999999999Z Not all systems will report fractions of a second at the same precision - it doesn't matter whether this is provided a millisecond or nanosecond precision, as long as it's provided.    message  A message describing the event.  The message  should not  contain metadata. Any metadata about the event should be presented in seperate fields.    logseverity  String value indicating the level of importance of the log message.  The  logseverity  specified in the log  must  be one of the following values;  TRACE ,  DEBUG ,  INFO ,  WARNING ,  ERROR  or  FATAL . The definition of each level is further described  here .       Additional fields will be required in specific contexts, and should be coordinated between services so that analysis can be correctly aggregated across different services.   If multiple services track e.g. session IDs, they should agree on a common field name, such as  session_id .  Case   word separation style must be consistent. The  snake_case  style should be preferred (e.g. use  session_id ,  not   SessionId )", 
            "title": "Logging"
        }, 
        {
            "location": "/documentation/contract/#logging-levels", 
            "text": "In this section each of the allowed log levels are specified and followed with a set of examples where each log level is applicable.   TRACE  Fine-grained debug message, typically used to capture a flow of events.  DEBUG  Mostly used for debugging purposes.  Typically not enabled by default in production.  INFO  Normal operations of the service that should be logged.  Indicate some important operation or state in the service.  WARNING  Indicate an event or state that was not part of common operations, but was handled.  Event that potentially can become an error.  Event that does not prevent correct operation of the system from an end-user perspective.  ERROR  Unhandled error that was unexpected to the service.  Service can continue normal operation after error recovery.  FATAL  Non-recoverable error that forces the service to terminate.", 
            "title": "Logging Levels"
        }, 
        {
            "location": "/documentation/contract/#health-checks", 
            "text": "todo", 
            "title": "Health Checks"
        }, 
        {
            "location": "/documentation/contract/#metrics", 
            "text": "todo", 
            "title": "Metrics"
        }, 
        {
            "location": "/documentation/recipes/overview/", 
            "text": "Recipes\n\n\nHere follows a brief overview of the recipes provided and the main questions each recipe addresses. Follow links into each recipe for more in-depth information.\n\n\nAuthorization\n\n\nHow do I make sure my users only see the data they are supposed to see?\n\n\nCore\n\n\nHow do I set up the core Frontira components in my orchestration platform of choice?\n\n\nData Loading\n\n\nHow do I load my own data into QIX Engine?\n\n\nDocument Distribution\n\n\nHow do I distribute updated documents/data in a cluster of QIX engine containers?\n\n\nLogging\n\n\nHow do I enable and configure logging in Frontira?\n\n\nMonitoring and Scaling\n\n\nHow do I monitor a system and take decisions on when to scale up or down QIX Engine instances?\n\n\nHow do I decide where to place QIX Engine sessions?", 
            "title": "Overview"
        }, 
        {
            "location": "/documentation/recipes/overview/#recipes", 
            "text": "Here follows a brief overview of the recipes provided and the main questions each recipe addresses. Follow links into each recipe for more in-depth information.", 
            "title": "Recipes"
        }, 
        {
            "location": "/documentation/recipes/overview/#authorization", 
            "text": "How do I make sure my users only see the data they are supposed to see?", 
            "title": "Authorization"
        }, 
        {
            "location": "/documentation/recipes/overview/#core", 
            "text": "How do I set up the core Frontira components in my orchestration platform of choice?", 
            "title": "Core"
        }, 
        {
            "location": "/documentation/recipes/overview/#data-loading", 
            "text": "How do I load my own data into QIX Engine?", 
            "title": "Data Loading"
        }, 
        {
            "location": "/documentation/recipes/overview/#document-distribution", 
            "text": "How do I distribute updated documents/data in a cluster of QIX engine containers?", 
            "title": "Document Distribution"
        }, 
        {
            "location": "/documentation/recipes/overview/#logging", 
            "text": "How do I enable and configure logging in Frontira?", 
            "title": "Logging"
        }, 
        {
            "location": "/documentation/recipes/overview/#monitoring-and-scaling", 
            "text": "How do I monitor a system and take decisions on when to scale up or down QIX Engine instances?  How do I decide where to place QIX Engine sessions?", 
            "title": "Monitoring and Scaling"
        }, 
        {
            "location": "/documentation/recipes/authorization/", 
            "text": "Authorization Recipe\n\n\nQuestion to answer: How can I make sure my users only see the data they are supposed to see?\n\n\nThere are several variants how to ensure data authorization:\n\n\n\n\nSegment apps / create apps per authorization target and route the user to the desired app\n\n\nOn-demand generation of apps, based on the authentication being provided (e.g. typical ODAG scenarios)\n\n\nUsing one app and use row-/column-based data authorization (section access) to serve the right data to the right audience\n\n\n\n\nThese topics does not necessarily require a lot of explanations, short examples/suggestions on how to approach it should be enough.\n\n\nInternal docs: https://confluence/x/6ZaBB", 
            "title": "Authorization"
        }, 
        {
            "location": "/documentation/recipes/authorization/#authorization-recipe", 
            "text": "Question to answer: How can I make sure my users only see the data they are supposed to see?  There are several variants how to ensure data authorization:   Segment apps / create apps per authorization target and route the user to the desired app  On-demand generation of apps, based on the authentication being provided (e.g. typical ODAG scenarios)  Using one app and use row-/column-based data authorization (section access) to serve the right data to the right audience   These topics does not necessarily require a lot of explanations, short examples/suggestions on how to approach it should be enough.  Internal docs: https://confluence/x/6ZaBB", 
            "title": "Authorization Recipe"
        }, 
        {
            "location": "/documentation/recipes/core/", 
            "text": "Core Recipe\n\n\nThis recipe demonstrates how to deploy a minimal Frontira stack using different orchestration tools. The purpose is to help you get started with Frontira.\n\n\nIt demonstrates a foundation to build full solutions on, where more aspects must also be considered, like authorization, document/data handling, vizualization, logging, monitoring etc. There are other recipes that cover such aspects separately.\n\n\nGitHub repo\n\n\nThe recipe assets are located at https://github.com/qlik-ea/core.\n\n\nWhen running commands presented in this recipe, it is assumed that they are run in that repository.\n\n\nContainer Orchestration\n\n\nThis recipe provides examples of deployment of the Frontira core services using the following container orchestration platforms:\n\n\n\n\nDocker Swarm\n\n\nKubernetes\n\n\nNomad\n (Experimental)\n\n\n\n\nServices\n\n\nThe core Frontira stack consists of the following services:\n\n\n\n\nQIX Engine\n\n\nLicense Service\n\n\nMira\n\n\n\n\nIn a typical solution, these are all deployed on the server side.\n\n\nLicensing\n\n\nSince QIX Engine runs under a license model, the examples require licensing configuration to be done. It should be clear in each example, and with the information provided below, how to do this.\n\n\nNOTE\n: The examples do not yet contain these configuration options since QIX Engine does not yet support it. Examples will be updated as soon as this becomes available.\n\n\nConfiguring the License Service\n\n\nThe License Service must be provided two environment variables that tells the service which license to use. These are:\n\n\n\n\nLEF_SERIAL\n - The LEF serial number which identifies the license to use.\n\n\nLEF_CONTROL\n - A control number used to validate the LEF serial number.\n\n\n\n\nBoth these will be provided by Qlik when receiving license details of the QIX Engine. In a docker-compose file, this could be done like this:\n\n\nversion: \n3.0\n\nservices:\n  license-service:\n    image: qlikea/license-service:\nversion\n\n    ports:\n      - 9200:9200\n    environment:\n      - LEF_SERIAL=\nLEF serial number here\n\n      - LEF_CONTROL=\nLEF control number here\n\n...      \n\n\n\n\nConfiguring the QIX Engine\n\n\nWhen running, the QIX Engine periodically communicates with the License Service to ensure that it is running under a valid license. QIX Engine deployments must be configured with the URL to use for accessing the Licence Service REST API. This is done by providing the \nLicenseServiceURL\n command switch to the engine. In a docker-compose file this would typically look like:\n\n\nversion: \n3.0\n\nservices:\n  qix-engine:\n    image: qlikea/engine:\nversion\n\n    ...\n    command: -S LicenseServiceURL=license-service:9200\n...\n\n\n\n\nDeploying to Docker Swarm\n\n\nPrerequisites\n\n\nIt is assumed that there is already a Docker Swarm cluster set up. If you would like to setup a local environment follow the \nInstall Docker\n guide.\n\n\nShell commands given below are assumed to be run in a Bash shell. On Windows, using \nGit Bash\n is recommended.\n\n\nYour Docker CLI must be set to issue commands to the swarm manager. This is typically done with:\n\n\n$ eval $(docker-machine env \nswarm manager node\n)\n\n\n\n\nThe Stack\n\n\nStack file\n\n\nThe Frontira stack, is specified in the \ndocker-compose.yml\n file. The stack consists of one QIX Engine, one Mira, and one License Service service.\n\n\nPlacement constraints\n\n\nThe Mira service is placed on the swarm manager node. It communicates with the manager Docker Engine. This is also why \n/var/run/docker.sock\n is mounted into the service.\n\n\nPorts\n\n\nMira and the License Service expose their REST APIs on ports \n9100\n and \n9200\n respectively. QIX Engine exposes port \n9076\n which serves the QIX websocket API and port \n9090\n which serves that Prometheus \n/metric\n endpoint separately.\n\n\nAlthough not strictly necessary, these ports are exposed to the outside of the swarm so that they can be easily accessed for demonstration purposes.\n\n\nLabeling\n\n\nAn important thing to note, is the label used on the QIX Engine service:\n\n\nlabels:\n  qix-engine: \n\n\n\n\n\nThis label is required for Mira to identify the engine service as a QIX Engine instance. The label to use is configurable and more information on this topic can be found in the \ndocumentation\n of Mira.\n\n\nDeploying\n\n\nThe stack can be deployed with the following command:\n\n\n$ docker stack deploy -c ./docker-compose.yml --with-registry-auth frontira\n\n\n\n\nNOTE\n: \n--with-registry-auth\n is currently needed since the Docker images being used are not public, and credentials are needed to pull them from Docker Hub.\n\n\nAccessing\n\n\nOnce the stack is deployed, it should be possible to list the tasks in it:\n\n\n$ docker stack ps frontira\n\n\n\n\nAlso, it should be possible to query Mira to return the list of QIX Engines it has discovered, by accessing its \n/engines\n endpoint:\n\n\n$ curl http://\nswarm manager node ip\n:9100/v1/engines\n\n\n\n\nThis should return JSON content containing one or more QIX Engine instance and information about these instances.\n\n\nDeploying to Kubernetes\n\n\nPrerequisites\n\n\nIt is assumed that there aleady is a Kubernetes cluster with one or more nodes. If you would like to setup a local single-node environment follow the \nInstall Minikube\n guide.\n\n\nShell commands given below are assumed to be run in a Bash shell. On Windows, using \nGit Bash\n is recommended.\n\n\nIt is assumed that \nkubectl\n is available in the local machine and that it is configured to communicate with the correct Kubernetes API server. Refer to the Kubernetes and Minikube documentation how to do this. When using Minikube this is often done automatically.\n\n\nDocker Hub Credentials\n\n\nSince the Docker images being used are not public you have to add a secret to Kubernetes to be able to pull these images from Docker Hub.\n\n\nTo add this secret to Kubernetes, the command below can be used.\n\n\nNOTE\n: The leading space character before the kubectl command is intentional. This prevents the command from being entered in the Bash shell command history, which is not desired for commands containing user credentials.\n\n\n$  kubectl create secret docker-registry dockerhub --docker-username=\nyour-name\n --docker-password=\nyour-password\n --docker-email=\nyour-email\n\n\n\n\n\nThe Stack\n\n\nThe Frontira stack consists of deployments and related services of QIX Engine, Mira, and the License Service.\n\n\nMira Kubernetes Mode\n\n\nIn the \nmira-deployment.yaml\n file, it is important to note a few things.\n\n\nObserve that the Mira deployment specifies two containers to run in the pod. This is because Mira needs to communicate with the Kubernetes API server through kubectl as a proxy.\n\n\nAlso note that Mira must be configured to run in Kubernetes mode. This does not happen automatically. To do this, the environment variable \nMIRA_MODE\n is set to \nkubernetes\n.\n\n\nServices\n\n\nmira\n and \nlicense-service\n, \nengine\n are configured services that exposes API ports to the outside of the Kubernetes cluster. Mira and the License Service have their REST APIs on ports \n9100\n and \n9200\n respectively. QIX Engine exposes port \n9076\n which serves the QIX websocket API and port \n9090\n which serves the Prometheus \n/metric\n endpoint separately.\n\n\nAlthough not strictly necessary, the services are exposed to the outside of the cluster so that they can be easily accessed for demonstration purposes.\n\n\nLabeling\n\n\nAn important thing to note, is the label used on the engine in the \nengine-deployment.yaml\n file:\n\n\nmetadata:\n  labels:\n    ...\n    qix-engine:\n\n\n\n\nThis label is required for Mira to identify engine pods as a QIX Engine instances. The label to use is configurable and more information on this topic can be found in the \ndocumentation\n of Mira.\n\n\nDeploying\n\n\nTwo types of deployments to Kubernetes are examplified. The first one is the \"plain\" Kubernetes method, using standard deployments and services. The second one shows how to use \nHelm\n, which provides a powerful way to manage Kubernetes applications.\n\n\nDeploying to plain Kubernetes\n\n\n$ cd kubernetes/plain\n$ kubectl create -f ./frontira/\n\n\n\n\nDeploying to Kubernetes with Helm\n\n\nHelm must be installed on the client side and Tiller on the server side. To enable Tiller follow this \nguide\n.\n\n\nTo deploy the stack using Helm, run:\n\n\n$ cd kubernetes/helm\n$ helm install ./frontira/\n\n\n\n\nAccessing\n\n\nOnce the stack is deployed, it should be possible to list all resources of the deployment with:\n\n\n$ kubectl get all\n\n\n\n\nAlso, it should be possible to query Mira to return the list of QIX Engines it has discovered, by accessing its \n/engines\n endpoint:\n\n\n$ curl http://\nkubernetes node ip\n:9100/v1/engines\n\n\n\n\nThis should return JSON content containing one QIX Engine instance and information about it.\n\n\nDeploying to Nomad\n\n\nNOTE\n: Deploying to Nomad is not officially supported by Frontira.\n\n\nIt is assumed that there is already a Nomad environment set up. If you would like to setup a local enviroment follow the \nInstall Nomad\n guide.\n\n\nSecrets\n\n\nSince the Docker images being used are not public you have to pass Docker credentials when deploying the Nomad stack. In this example we use the Docker auths that is stored when logging into docker hub i.e. \ndocker login\n.\n\n\nBe aware that the Docker credentials are stored in plain text in Nomad, see \nhere\n. In \nnomad.hcl\n there is an example of how the Nomad client can be configured to use local docker credentials.\n\n\nService Discovery\n\n\nMira uses the DNS mode for service discovery in a Nomad orchestration. This assumes that there is a running Consul server in the Nomad environment. Nomad will automatically register services in Consul when deploying the \nnomad\n files. The hostname that Mira should use for discovering QIX Engine instances is defined in the task configuration for \nMira\n.\n\n\nDeploy\n\n\nDeploy the core stack with the following commands:\n\n\n$ nomad run ./nomad/mira.nomad\n$ nomad run ./nomad/license-service.nomad\n$ nomad run ./nomad/engine.nomad", 
            "title": "Core"
        }, 
        {
            "location": "/documentation/recipes/core/#core-recipe", 
            "text": "This recipe demonstrates how to deploy a minimal Frontira stack using different orchestration tools. The purpose is to help you get started with Frontira.  It demonstrates a foundation to build full solutions on, where more aspects must also be considered, like authorization, document/data handling, vizualization, logging, monitoring etc. There are other recipes that cover such aspects separately.", 
            "title": "Core Recipe"
        }, 
        {
            "location": "/documentation/recipes/core/#github-repo", 
            "text": "The recipe assets are located at https://github.com/qlik-ea/core.  When running commands presented in this recipe, it is assumed that they are run in that repository.", 
            "title": "GitHub repo"
        }, 
        {
            "location": "/documentation/recipes/core/#container-orchestration", 
            "text": "This recipe provides examples of deployment of the Frontira core services using the following container orchestration platforms:   Docker Swarm  Kubernetes  Nomad  (Experimental)", 
            "title": "Container Orchestration"
        }, 
        {
            "location": "/documentation/recipes/core/#services", 
            "text": "The core Frontira stack consists of the following services:   QIX Engine  License Service  Mira   In a typical solution, these are all deployed on the server side.", 
            "title": "Services"
        }, 
        {
            "location": "/documentation/recipes/core/#licensing", 
            "text": "Since QIX Engine runs under a license model, the examples require licensing configuration to be done. It should be clear in each example, and with the information provided below, how to do this.  NOTE : The examples do not yet contain these configuration options since QIX Engine does not yet support it. Examples will be updated as soon as this becomes available.", 
            "title": "Licensing"
        }, 
        {
            "location": "/documentation/recipes/core/#configuring-the-license-service", 
            "text": "The License Service must be provided two environment variables that tells the service which license to use. These are:   LEF_SERIAL  - The LEF serial number which identifies the license to use.  LEF_CONTROL  - A control number used to validate the LEF serial number.   Both these will be provided by Qlik when receiving license details of the QIX Engine. In a docker-compose file, this could be done like this:  version:  3.0 \nservices:\n  license-service:\n    image: qlikea/license-service: version \n    ports:\n      - 9200:9200\n    environment:\n      - LEF_SERIAL= LEF serial number here \n      - LEF_CONTROL= LEF control number here \n...", 
            "title": "Configuring the License Service"
        }, 
        {
            "location": "/documentation/recipes/core/#configuring-the-qix-engine", 
            "text": "When running, the QIX Engine periodically communicates with the License Service to ensure that it is running under a valid license. QIX Engine deployments must be configured with the URL to use for accessing the Licence Service REST API. This is done by providing the  LicenseServiceURL  command switch to the engine. In a docker-compose file this would typically look like:  version:  3.0 \nservices:\n  qix-engine:\n    image: qlikea/engine: version \n    ...\n    command: -S LicenseServiceURL=license-service:9200\n...", 
            "title": "Configuring the QIX Engine"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying-to-docker-swarm", 
            "text": "", 
            "title": "Deploying to Docker Swarm"
        }, 
        {
            "location": "/documentation/recipes/core/#prerequisites", 
            "text": "It is assumed that there is already a Docker Swarm cluster set up. If you would like to setup a local environment follow the  Install Docker  guide.  Shell commands given below are assumed to be run in a Bash shell. On Windows, using  Git Bash  is recommended.  Your Docker CLI must be set to issue commands to the swarm manager. This is typically done with:  $ eval $(docker-machine env  swarm manager node )", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/documentation/recipes/core/#the-stack", 
            "text": "", 
            "title": "The Stack"
        }, 
        {
            "location": "/documentation/recipes/core/#stack-file", 
            "text": "The Frontira stack, is specified in the  docker-compose.yml  file. The stack consists of one QIX Engine, one Mira, and one License Service service.", 
            "title": "Stack file"
        }, 
        {
            "location": "/documentation/recipes/core/#placement-constraints", 
            "text": "The Mira service is placed on the swarm manager node. It communicates with the manager Docker Engine. This is also why  /var/run/docker.sock  is mounted into the service.", 
            "title": "Placement constraints"
        }, 
        {
            "location": "/documentation/recipes/core/#ports", 
            "text": "Mira and the License Service expose their REST APIs on ports  9100  and  9200  respectively. QIX Engine exposes port  9076  which serves the QIX websocket API and port  9090  which serves that Prometheus  /metric  endpoint separately.  Although not strictly necessary, these ports are exposed to the outside of the swarm so that they can be easily accessed for demonstration purposes.", 
            "title": "Ports"
        }, 
        {
            "location": "/documentation/recipes/core/#labeling", 
            "text": "An important thing to note, is the label used on the QIX Engine service:  labels:\n  qix-engine:    This label is required for Mira to identify the engine service as a QIX Engine instance. The label to use is configurable and more information on this topic can be found in the  documentation  of Mira.", 
            "title": "Labeling"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying", 
            "text": "The stack can be deployed with the following command:  $ docker stack deploy -c ./docker-compose.yml --with-registry-auth frontira  NOTE :  --with-registry-auth  is currently needed since the Docker images being used are not public, and credentials are needed to pull them from Docker Hub.", 
            "title": "Deploying"
        }, 
        {
            "location": "/documentation/recipes/core/#accessing", 
            "text": "Once the stack is deployed, it should be possible to list the tasks in it:  $ docker stack ps frontira  Also, it should be possible to query Mira to return the list of QIX Engines it has discovered, by accessing its  /engines  endpoint:  $ curl http:// swarm manager node ip :9100/v1/engines  This should return JSON content containing one or more QIX Engine instance and information about these instances.", 
            "title": "Accessing"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying-to-kubernetes", 
            "text": "", 
            "title": "Deploying to Kubernetes"
        }, 
        {
            "location": "/documentation/recipes/core/#prerequisites_1", 
            "text": "It is assumed that there aleady is a Kubernetes cluster with one or more nodes. If you would like to setup a local single-node environment follow the  Install Minikube  guide.  Shell commands given below are assumed to be run in a Bash shell. On Windows, using  Git Bash  is recommended.  It is assumed that  kubectl  is available in the local machine and that it is configured to communicate with the correct Kubernetes API server. Refer to the Kubernetes and Minikube documentation how to do this. When using Minikube this is often done automatically.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/documentation/recipes/core/#docker-hub-credentials", 
            "text": "Since the Docker images being used are not public you have to add a secret to Kubernetes to be able to pull these images from Docker Hub.  To add this secret to Kubernetes, the command below can be used.  NOTE : The leading space character before the kubectl command is intentional. This prevents the command from being entered in the Bash shell command history, which is not desired for commands containing user credentials.  $  kubectl create secret docker-registry dockerhub --docker-username= your-name  --docker-password= your-password  --docker-email= your-email", 
            "title": "Docker Hub Credentials"
        }, 
        {
            "location": "/documentation/recipes/core/#the-stack_1", 
            "text": "The Frontira stack consists of deployments and related services of QIX Engine, Mira, and the License Service.", 
            "title": "The Stack"
        }, 
        {
            "location": "/documentation/recipes/core/#mira-kubernetes-mode", 
            "text": "In the  mira-deployment.yaml  file, it is important to note a few things.  Observe that the Mira deployment specifies two containers to run in the pod. This is because Mira needs to communicate with the Kubernetes API server through kubectl as a proxy.  Also note that Mira must be configured to run in Kubernetes mode. This does not happen automatically. To do this, the environment variable  MIRA_MODE  is set to  kubernetes .", 
            "title": "Mira Kubernetes Mode"
        }, 
        {
            "location": "/documentation/recipes/core/#services_1", 
            "text": "mira  and  license-service ,  engine  are configured services that exposes API ports to the outside of the Kubernetes cluster. Mira and the License Service have their REST APIs on ports  9100  and  9200  respectively. QIX Engine exposes port  9076  which serves the QIX websocket API and port  9090  which serves the Prometheus  /metric  endpoint separately.  Although not strictly necessary, the services are exposed to the outside of the cluster so that they can be easily accessed for demonstration purposes.", 
            "title": "Services"
        }, 
        {
            "location": "/documentation/recipes/core/#labeling_1", 
            "text": "An important thing to note, is the label used on the engine in the  engine-deployment.yaml  file:  metadata:\n  labels:\n    ...\n    qix-engine:  This label is required for Mira to identify engine pods as a QIX Engine instances. The label to use is configurable and more information on this topic can be found in the  documentation  of Mira.", 
            "title": "Labeling"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying_1", 
            "text": "Two types of deployments to Kubernetes are examplified. The first one is the \"plain\" Kubernetes method, using standard deployments and services. The second one shows how to use  Helm , which provides a powerful way to manage Kubernetes applications.", 
            "title": "Deploying"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying-to-plain-kubernetes", 
            "text": "$ cd kubernetes/plain\n$ kubectl create -f ./frontira/", 
            "title": "Deploying to plain Kubernetes"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying-to-kubernetes-with-helm", 
            "text": "Helm must be installed on the client side and Tiller on the server side. To enable Tiller follow this  guide .  To deploy the stack using Helm, run:  $ cd kubernetes/helm\n$ helm install ./frontira/", 
            "title": "Deploying to Kubernetes with Helm"
        }, 
        {
            "location": "/documentation/recipes/core/#accessing_1", 
            "text": "Once the stack is deployed, it should be possible to list all resources of the deployment with:  $ kubectl get all  Also, it should be possible to query Mira to return the list of QIX Engines it has discovered, by accessing its  /engines  endpoint:  $ curl http:// kubernetes node ip :9100/v1/engines  This should return JSON content containing one QIX Engine instance and information about it.", 
            "title": "Accessing"
        }, 
        {
            "location": "/documentation/recipes/core/#deploying-to-nomad", 
            "text": "NOTE : Deploying to Nomad is not officially supported by Frontira.  It is assumed that there is already a Nomad environment set up. If you would like to setup a local enviroment follow the  Install Nomad  guide.", 
            "title": "Deploying to Nomad"
        }, 
        {
            "location": "/documentation/recipes/core/#secrets", 
            "text": "Since the Docker images being used are not public you have to pass Docker credentials when deploying the Nomad stack. In this example we use the Docker auths that is stored when logging into docker hub i.e.  docker login .  Be aware that the Docker credentials are stored in plain text in Nomad, see  here . In  nomad.hcl  there is an example of how the Nomad client can be configured to use local docker credentials.", 
            "title": "Secrets"
        }, 
        {
            "location": "/documentation/recipes/core/#service-discovery", 
            "text": "Mira uses the DNS mode for service discovery in a Nomad orchestration. This assumes that there is a running Consul server in the Nomad environment. Nomad will automatically register services in Consul when deploying the  nomad  files. The hostname that Mira should use for discovering QIX Engine instances is defined in the task configuration for  Mira .", 
            "title": "Service Discovery"
        }, 
        {
            "location": "/documentation/recipes/core/#deploy", 
            "text": "Deploy the core stack with the following commands:  $ nomad run ./nomad/mira.nomad\n$ nomad run ./nomad/license-service.nomad\n$ nomad run ./nomad/engine.nomad", 
            "title": "Deploy"
        }, 
        {
            "location": "/documentation/recipes/data-loading/", 
            "text": "Data Loading Recipe\n\n\nThis recipe outlines a couple of different approaches and things to keep in mind when loading data into your document\nin a scalable environment.\n\n\nEnd-user defined data\n\n\nThis section is mainly documentation how to use the halyard.js stream service to allow customized end-user\ndata through services like Dropbox.\n\n\nDeveloper defined data\n\n\nThis section contains a runnable example how to load some airport data from a postgres database using the GRPC protocol in the QIX Engine. \nIt assumes you are running in a *nix environment or use Git Bash on Windows. Basic Docker knowledge is also assumed.\n\n\nFirst clone and open the \npostgres-grpc-connector\n repository and move to the \nexample\n folder.\n\n\nThen we need to build our postgres image containing our airport data.\n\n\n$ cd postgres-image\n$ docker build . -t example/postgres-grpc-connector-database\n\n\n\n\nThe \nDockerfile\n builds on the official postgres image and then copies our \nairports.csv\n file into the image as well as copying the \ninit-airports-data.sql\n script to the folder \ndocker-entrypoint-initdb.d\n on the image. \n\n\nWhen the postgres image starts it will run all \n.sql\n files inside the folder and thus our \ninit-airports-data.sql\n will be run.\n\n\nThe \nscript\n creates a table in the default database containing the data from the copied \nairports.csv\n file. Thus when running the created image we will have a standard postgres database with an airports table containing our data from the \nairports.csv\n file.\n\n\nAfter building our database image we can start our containers.\n\n\n$ cd ..\n$ docker-compose up -d\n\n\n\n\nYou can see that the \ndocker-compose\n starts three services. The first is the database using the image we just created. The second is the postgres-grp-connector and the final one is the QIX Engine. \n\n\nFor the QIX Engine we have opened and mapped port \n9076\n on the container to port \n9076\n on our local machine, this is because we will access the QIX Engine from outside of the docker network to trigger a load of our airport data and thus the port needs to open to the outside.\n\n\nWe have also added a few commands to the QIX Engine \n-S EnableGrpcCustomConnectors=1\n enables grpc-connectors in the QIX Engine and \n-S GrpcConnectorPlugins=\"postgres-grpc-connector,postgres-grpc-connector:50051\"\n creates a connector of type \npostgres-grpc-connector\n which we tell the QIX Engine exists on host \npostgres-grpc-connector\n and listening on port \n50051\n\n\nWe can see our three running containers using \n\n\n$ docker ps\n\n\n\n\nLooking at the ports here we can see that the database container exposes port \n5432\n on the network the containers share among themselves and the \npostgres-grpc-connector\n can therefore access to the database container on this port. \n\n\nThe \npostgres-grpc-connector\n container exposes the port \n50051\n which is the same port we told the QIX Engine to talk to it on with the commands in the \ndocker-compose\n file.\n\n\nFinally we can see that the engine exposes the ports \n9076\n and \n9090\n. The \n9090\n port is used for metrics and is not interesting in this example. \n\n\nBut the \n9076\n port is the QIX Engines standard API port and here we have also opened it to the \noutside\n and mapped it to port \n9076\n on your local machine. So requests to your machine on port \n9076\n will go to QIX Engine container.\n\n\nNow that we have a database container with the data, a GRPC-Connector container and a QIX Engine running all we need is to trigger a load of the data.\n\n\nWe will do this with a small \nNode.JS\n program using the Qlik library \nenigma.js\n to talk with the QIX Engine to trigger a load of our airport data via the GRPC-Connector.\n\n\n$ cd reload-runner\n$ npm install\n$ npm start\n\n\n\n\nYou should see the info from 10 different airports in your terminal.\n\n\nWhat the program does is it first creates or opens an app called \nreloadapp.qvf\n on the QIX Engine. Then it creates a connection of the type we defined earlier\n\n\napp.createConnection({\n    qType: 'postgres-grpc-connector', //the name we defined as a parameter to the QIX Engine in our docker-compose.yml\n    qName: 'postgresgrpc',\n    qConnectionString: 'CUSTOM CONNECT TO \nprovider=postgres-grpc-connector;host=postgres-database;port=5432;database=postgres\n', //the connection string inclues both the provider to use and parameters to it.\n    qUserName: 'postgres', //username and password for the postgres database, provided to the grpc-connector\n    qPassword: 'postgres'\n});\n\n\n\n\nThe connection string \nCUSTOM CONNECT TO \"provider=postgres-grpc-connector;host=postgres-database;port=5432;database=postgres\n tells the QIX Engine to use the \npostgres-grpc-connector\n and then the rest is parameters to the GRPC-Connector such as the database host address and port. You can see that the host is the name of the service we defined in the \ndocker-compose\n and the port is the port the database-container exposes.\n\n\nAfter this we set a script to use the connection we just created.\n\n\nconst script = `\n    lib connect to 'postgresgrpc';      \n    Airports:                       \n    sql select rowID,Airport,City,Country,IATACode,ICAOCode,Latitude,Longitude,Altitude,TimeZone,DST,TZ, clock_timestamp() from airports;\n`; // add script to use the grpc-connector and load a table\napp.setScript(script);\n\n\n\n\nWe are using the name \npostgresgrpc\n that we defined when we created our connection and then we are loading the airport data from the postgres \nairports\n table into the qix table \nAirports\n\n\nThen we do a reload to load the new data into the QIX Engine and after that we fetch the first 10 results of the \nAirports\n table and print them to the terminal.\n\n\nYou can get more detailed info by looking inside the \nindex.js\n file and reading on \nenigma.js\n.", 
            "title": "Data Loading"
        }, 
        {
            "location": "/documentation/recipes/data-loading/#data-loading-recipe", 
            "text": "This recipe outlines a couple of different approaches and things to keep in mind when loading data into your document\nin a scalable environment.", 
            "title": "Data Loading Recipe"
        }, 
        {
            "location": "/documentation/recipes/data-loading/#end-user-defined-data", 
            "text": "This section is mainly documentation how to use the halyard.js stream service to allow customized end-user\ndata through services like Dropbox.", 
            "title": "End-user defined data"
        }, 
        {
            "location": "/documentation/recipes/data-loading/#developer-defined-data", 
            "text": "This section contains a runnable example how to load some airport data from a postgres database using the GRPC protocol in the QIX Engine. \nIt assumes you are running in a *nix environment or use Git Bash on Windows. Basic Docker knowledge is also assumed.  First clone and open the  postgres-grpc-connector  repository and move to the  example  folder.  Then we need to build our postgres image containing our airport data.  $ cd postgres-image\n$ docker build . -t example/postgres-grpc-connector-database  The  Dockerfile  builds on the official postgres image and then copies our  airports.csv  file into the image as well as copying the  init-airports-data.sql  script to the folder  docker-entrypoint-initdb.d  on the image.   When the postgres image starts it will run all  .sql  files inside the folder and thus our  init-airports-data.sql  will be run.  The  script  creates a table in the default database containing the data from the copied  airports.csv  file. Thus when running the created image we will have a standard postgres database with an airports table containing our data from the  airports.csv  file.  After building our database image we can start our containers.  $ cd ..\n$ docker-compose up -d  You can see that the  docker-compose  starts three services. The first is the database using the image we just created. The second is the postgres-grp-connector and the final one is the QIX Engine.   For the QIX Engine we have opened and mapped port  9076  on the container to port  9076  on our local machine, this is because we will access the QIX Engine from outside of the docker network to trigger a load of our airport data and thus the port needs to open to the outside.  We have also added a few commands to the QIX Engine  -S EnableGrpcCustomConnectors=1  enables grpc-connectors in the QIX Engine and  -S GrpcConnectorPlugins=\"postgres-grpc-connector,postgres-grpc-connector:50051\"  creates a connector of type  postgres-grpc-connector  which we tell the QIX Engine exists on host  postgres-grpc-connector  and listening on port  50051  We can see our three running containers using   $ docker ps  Looking at the ports here we can see that the database container exposes port  5432  on the network the containers share among themselves and the  postgres-grpc-connector  can therefore access to the database container on this port.   The  postgres-grpc-connector  container exposes the port  50051  which is the same port we told the QIX Engine to talk to it on with the commands in the  docker-compose  file.  Finally we can see that the engine exposes the ports  9076  and  9090 . The  9090  port is used for metrics and is not interesting in this example.   But the  9076  port is the QIX Engines standard API port and here we have also opened it to the  outside  and mapped it to port  9076  on your local machine. So requests to your machine on port  9076  will go to QIX Engine container.  Now that we have a database container with the data, a GRPC-Connector container and a QIX Engine running all we need is to trigger a load of the data.  We will do this with a small  Node.JS  program using the Qlik library  enigma.js  to talk with the QIX Engine to trigger a load of our airport data via the GRPC-Connector.  $ cd reload-runner\n$ npm install\n$ npm start  You should see the info from 10 different airports in your terminal.  What the program does is it first creates or opens an app called  reloadapp.qvf  on the QIX Engine. Then it creates a connection of the type we defined earlier  app.createConnection({\n    qType: 'postgres-grpc-connector', //the name we defined as a parameter to the QIX Engine in our docker-compose.yml\n    qName: 'postgresgrpc',\n    qConnectionString: 'CUSTOM CONNECT TO  provider=postgres-grpc-connector;host=postgres-database;port=5432;database=postgres ', //the connection string inclues both the provider to use and parameters to it.\n    qUserName: 'postgres', //username and password for the postgres database, provided to the grpc-connector\n    qPassword: 'postgres'\n});  The connection string  CUSTOM CONNECT TO \"provider=postgres-grpc-connector;host=postgres-database;port=5432;database=postgres  tells the QIX Engine to use the  postgres-grpc-connector  and then the rest is parameters to the GRPC-Connector such as the database host address and port. You can see that the host is the name of the service we defined in the  docker-compose  and the port is the port the database-container exposes.  After this we set a script to use the connection we just created.  const script = `\n    lib connect to 'postgresgrpc';      \n    Airports:                       \n    sql select rowID,Airport,City,Country,IATACode,ICAOCode,Latitude,Longitude,Altitude,TimeZone,DST,TZ, clock_timestamp() from airports;\n`; // add script to use the grpc-connector and load a table\napp.setScript(script);  We are using the name  postgresgrpc  that we defined when we created our connection and then we are loading the airport data from the postgres  airports  table into the qix table  Airports  Then we do a reload to load the new data into the QIX Engine and after that we fetch the first 10 results of the  Airports  table and print them to the terminal.  You can get more detailed info by looking inside the  index.js  file and reading on  enigma.js .", 
            "title": "Developer defined data"
        }, 
        {
            "location": "/documentation/recipes/document-distribution/", 
            "text": "Document Distribution Recipe\n\n\nTODO...", 
            "title": "Document Distribution"
        }, 
        {
            "location": "/documentation/recipes/document-distribution/#document-distribution-recipe", 
            "text": "TODO...", 
            "title": "Document Distribution Recipe"
        }, 
        {
            "location": "/documentation/recipes/logging/", 
            "text": "Logging Recipe\n\n\nQuestion to answer: How do I enable and configure logs in Frontira?\n\n\n\n\nDescribe how to config Frontira services for logging, i.e. how do I enable the logs, specify output format and log levels.\n\n\nProvide specification on the logging format\n\n\n\n\nThe recipe does NOT contain:\n\n\n\n\nSpecific logging stacks like ELK\n\n\nSpecific dashboards for e.g. Kibana\n\n\n\n\nInternal docs: https://confluence/x/ZZaBB", 
            "title": "Logging"
        }, 
        {
            "location": "/documentation/recipes/logging/#logging-recipe", 
            "text": "Question to answer: How do I enable and configure logs in Frontira?   Describe how to config Frontira services for logging, i.e. how do I enable the logs, specify output format and log levels.  Provide specification on the logging format   The recipe does NOT contain:   Specific logging stacks like ELK  Specific dashboards for e.g. Kibana   Internal docs: https://confluence/x/ZZaBB", 
            "title": "Logging Recipe"
        }, 
        {
            "location": "/documentation/recipes/monitoring-and-scaling/", 
            "text": "Monitoring and Scaling Recipe\n\n\nQuestions to answer:\n\n\n\n\nHow do I get an overview of my system?\n  What metrics are interesting to look at?\n\n\nHow do I know where to place a session?\n  Which metrics can be used for session placement, describe a couple of scenarios.\n\n\nHow do I know when to scale up and down?\n  Good indicators for when I need to scale up and down, describe a couple of scenarios.\n\n\nHow do I know what system (\u201csize\u201d of infrastructure) I need?\n  Algorithm that can be applied to get a rough estimation (e.g. container memory when doc is loaded, add 10% per session)?\n\n\n\n\nInternal docs: https://confluence/x/UJmBB", 
            "title": "Monitoring and Scaling"
        }, 
        {
            "location": "/documentation/recipes/monitoring-and-scaling/#monitoring-and-scaling-recipe", 
            "text": "Questions to answer:   How do I get an overview of my system?\n  What metrics are interesting to look at?  How do I know where to place a session?\n  Which metrics can be used for session placement, describe a couple of scenarios.  How do I know when to scale up and down?\n  Good indicators for when I need to scale up and down, describe a couple of scenarios.  How do I know what system (\u201csize\u201d of infrastructure) I need?\n  Algorithm that can be applied to get a rough estimation (e.g. container memory when doc is loaded, add 10% per session)?   Internal docs: https://confluence/x/UJmBB", 
            "title": "Monitoring and Scaling Recipe"
        }, 
        {
            "location": "/documentation/apis/", 
            "text": "TODO\n\nNeed to figure out if and how to host API of services here", 
            "title": "APIs"
        }, 
        {
            "location": "/use-cases/overview/", 
            "text": "Use Cases\n\n\nUse cases provide larger end-to-end examples of how Frontira can be used to build applications that leverage the power of the QIX Engine and related services.\n\n\nCurrently, one use case is provided:\n\n\n\n\nCustom Analytics UI", 
            "title": "Overview"
        }, 
        {
            "location": "/use-cases/overview/#use-cases", 
            "text": "Use cases provide larger end-to-end examples of how Frontira can be used to build applications that leverage the power of the QIX Engine and related services.  Currently, one use case is provided:   Custom Analytics UI", 
            "title": "Use Cases"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/", 
            "text": "Use Case - Custom Analytics UI\n\n\nWORK IN PROGRESS\n\n\nBackground\n\n\nA company is opening a medical data portal. This portal proposes some advanced analysis capabilities on drugs/treatment/reactions. It targets the world-wide population of doctors and works with an annual subscription. Even if the audience is more or less predictable, some seasonal or sudden epidemic events can affect the traffic. With the auto-medication trend, the company also plan to open this service to the public.\n\n\nBusiness Requirements\n\n\n\n\nThe portal should be able to serve peak traffic of around 10.000 simultaneous connections with an average of 500 on the given data model.\n\n\nThe portal should run with minimal downtime, rolling updates of the web application should be possible without any downtime.\n\n\n\n\nTechnical Requirements\n\n\n\n\nThe company is already using Docker and Docker Swarm Mode in complementary backend system of this portal.\n\n\nAWS is the preferred cloud provider, although the implementation should allow to move to another cloud provider (e.g. DigitalOcean) or to and on-prem deployment with minimal efforts.\n\n\nThe implementation should meet industry best practices in terms of being able to monitor the system.\n\n\n\n\nFunctional Requirements\n\n\n\n\nAs a dev-ops I want to provision and run docker nodes hosting the portal so that I can support peak traffic around 10.000 simultaneous connections and an average of 500 on the given data model.\n\n\nAs a dev-ops I want to be able to update my system without interruption of the service.\n\n\nAs an end-user I want to be able to use a UI tailored for my needs so I can quickly find the insights I need.\n\n\nAs an end-user I want to be able to stay logged in so that I can access the portal conveniently.\n\n\n\n\nAssumptions\n\n\n\n\nInitially, all users need to be logged in to use the portal.\n\n\nThe scaling will be done manually with the help of scripts and will depend on the anticipated traffic.\n\n\nThe data set (no dynamic data reduction) is the same for every end-user.\n\n\nThe data reload is usually every quarter when FDA releases them.\n\n\nA subscription model won't be implemented (rely on authentication permissions only).\n\n\n\n\nData\n\n\nThis use case is characterized by a \nsingle qvf\n with the following data model:\n\n\n\n\nUI\n\n\nThe main benefit for doctors is to be able to narrow analysis based on advanced collection of demographic criteria (gender, weight, origin etc.).\n\n\nThe web application presents information in four main tabs focusing on:\n\n\n\n\nfilters\n\n\nprescription viz/table\n\n\nside effects/reactions viz/table\n\n\nreport\n\n\n\n\n(There will be multiple objects on each single page)\n\n\n\n\nDetailed Functional Requirements\n\n\nScaling\n\n\n\n\nWhile it is a basic assumption of this use-case, that scaling is done manually, I'd like to know as a developer where I could hook into the APIs to create my custom, fully automated scaling strategy.\n\n\n\n\nMonitoring\n\n\n\n\nAs a dev-ops I want to be able to monitor the system and find potential issues.\n\n\nAs a dev-ops I want to be able to monitor all log-files from all services/containers.\n\n\nAs a dev-ops I want to be able to analyze the number of page-hits/sessions over time.\n\n\nAs a dev-ops I want to be able to get typical web-page KPIs out of the system (page-hits, sessions, up-time, down-time, reliability, etc.)\n\n\nAs a dev-ops I want to be able to get some information from the monitoring-system about the whether I should scale up/down the system (based on the assumption mentioned above that scaling is done manually)\n\n\nAs a dev-ops I want to be able to see how the QiX Engine containers are behaving, including getting detailed log-files and error-messages\n\n\n\n\nTesting\n\n\n\n\nAs a dev-ops I expect basic e2e tests\n\n\nAs a dev-ops I expect stress-tests to find out the limits of the given setup (machines \n number of distributed services)\n\n\nMax requests/hits handled per sec\n\n\nFailure rate / Errors per second\n\n\nAvg/Min/Max response time\n\n\nLatency\n\n\nNumber of users handled by the system\n\n\nSee e.g. \nthis article\n for more examples\n\n\n\n\n\n\nWhen using the stress-tests mentioned above, I - as a dev-ops - expect to be able to configure key-settings, which drive the behavior of the stress-test\n\n\nNumber of concurrent users\n\n\nActivity pattern of users (just watching, heavily making selections, etc.)\n\n\n\n\n\n\n\n\nSystem Design\n\n\nNOTE: The design documentation is work in progress. More information will be added on a regular basis.\n\n\nThis use case is about scaling the QIX Engine in a configuration of\n- One document\n- Multiple users\n\n\nScaling up engines needs to be done only to reduce load as a consequence of multiple users access the system simultaneously. All engine instances are equivalent and there is no need to have a certain engine service a certain user since all users access the same single document.\n\n\nMore information here\n\n\nQIX Engine Session Management", 
            "title": "Custom Analytics UI"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#use-case-custom-analytics-ui", 
            "text": "WORK IN PROGRESS", 
            "title": "Use Case - Custom Analytics UI"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#background", 
            "text": "A company is opening a medical data portal. This portal proposes some advanced analysis capabilities on drugs/treatment/reactions. It targets the world-wide population of doctors and works with an annual subscription. Even if the audience is more or less predictable, some seasonal or sudden epidemic events can affect the traffic. With the auto-medication trend, the company also plan to open this service to the public.", 
            "title": "Background"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#business-requirements", 
            "text": "The portal should be able to serve peak traffic of around 10.000 simultaneous connections with an average of 500 on the given data model.  The portal should run with minimal downtime, rolling updates of the web application should be possible without any downtime.", 
            "title": "Business Requirements"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#technical-requirements", 
            "text": "The company is already using Docker and Docker Swarm Mode in complementary backend system of this portal.  AWS is the preferred cloud provider, although the implementation should allow to move to another cloud provider (e.g. DigitalOcean) or to and on-prem deployment with minimal efforts.  The implementation should meet industry best practices in terms of being able to monitor the system.", 
            "title": "Technical Requirements"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#functional-requirements", 
            "text": "As a dev-ops I want to provision and run docker nodes hosting the portal so that I can support peak traffic around 10.000 simultaneous connections and an average of 500 on the given data model.  As a dev-ops I want to be able to update my system without interruption of the service.  As an end-user I want to be able to use a UI tailored for my needs so I can quickly find the insights I need.  As an end-user I want to be able to stay logged in so that I can access the portal conveniently.", 
            "title": "Functional Requirements"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#assumptions", 
            "text": "Initially, all users need to be logged in to use the portal.  The scaling will be done manually with the help of scripts and will depend on the anticipated traffic.  The data set (no dynamic data reduction) is the same for every end-user.  The data reload is usually every quarter when FDA releases them.  A subscription model won't be implemented (rely on authentication permissions only).", 
            "title": "Assumptions"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#data", 
            "text": "This use case is characterized by a  single qvf  with the following data model:", 
            "title": "Data"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#ui", 
            "text": "The main benefit for doctors is to be able to narrow analysis based on advanced collection of demographic criteria (gender, weight, origin etc.).  The web application presents information in four main tabs focusing on:   filters  prescription viz/table  side effects/reactions viz/table  report   (There will be multiple objects on each single page)", 
            "title": "UI"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#detailed-functional-requirements", 
            "text": "", 
            "title": "Detailed Functional Requirements"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#scaling", 
            "text": "While it is a basic assumption of this use-case, that scaling is done manually, I'd like to know as a developer where I could hook into the APIs to create my custom, fully automated scaling strategy.", 
            "title": "Scaling"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#monitoring", 
            "text": "As a dev-ops I want to be able to monitor the system and find potential issues.  As a dev-ops I want to be able to monitor all log-files from all services/containers.  As a dev-ops I want to be able to analyze the number of page-hits/sessions over time.  As a dev-ops I want to be able to get typical web-page KPIs out of the system (page-hits, sessions, up-time, down-time, reliability, etc.)  As a dev-ops I want to be able to get some information from the monitoring-system about the whether I should scale up/down the system (based on the assumption mentioned above that scaling is done manually)  As a dev-ops I want to be able to see how the QiX Engine containers are behaving, including getting detailed log-files and error-messages", 
            "title": "Monitoring"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#testing", 
            "text": "As a dev-ops I expect basic e2e tests  As a dev-ops I expect stress-tests to find out the limits of the given setup (machines   number of distributed services)  Max requests/hits handled per sec  Failure rate / Errors per second  Avg/Min/Max response time  Latency  Number of users handled by the system  See e.g.  this article  for more examples    When using the stress-tests mentioned above, I - as a dev-ops - expect to be able to configure key-settings, which drive the behavior of the stress-test  Number of concurrent users  Activity pattern of users (just watching, heavily making selections, etc.)", 
            "title": "Testing"
        }, 
        {
            "location": "/use-cases/use-case-custom-analytics/README/#system-design", 
            "text": "NOTE: The design documentation is work in progress. More information will be added on a regular basis.  This use case is about scaling the QIX Engine in a configuration of\n- One document\n- Multiple users  Scaling up engines needs to be done only to reduce load as a consequence of multiple users access the system simultaneously. All engine instances are equivalent and there is no need to have a certain engine service a certain user since all users access the same single document.  More information here  QIX Engine Session Management", 
            "title": "System Design"
        }, 
        {
            "location": "/terminology/", 
            "text": "Terminology\n\n\nThis pages contains terminology used around elastic, docker, orchestration, and deployment to get a common understanding used among teams, partners and customers.\n\n\nD\n\n\nDocker swarm\n - Docker native clustering system\n\n\nE\n\n\nElasticsearch\n - Open Source, Distributed, RESTful Search Engine\n\n\nEnigma\n - Framework used for communicating with qlik engine.\n\n\nH\n\n\nHalyard\n - Library used for loading data into qlik engine.\n\n\nK\n\n\nKibana\n - Browser based analytics search dashboard for \nElasticsearch\n\n\nKubernetes\n - Automated container deployment, scaling, and management\n\n\nL\n\n\nLogstash\n - Processes and transforms data, and sends it to e.g. \nElasticsearch\n.\n\n\nM\n\n\nMarathon\n - A container orchestration platform for Mesos.\n\n\nMira\n - A QIX Engine strategy service for Qlik Elastic.\n\n\nN\n\n\nNginx\n - HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server\n\n\nT\n\n\nTraefik\n - HTTP reverse proxy and load balancer.", 
            "title": "Terminology"
        }, 
        {
            "location": "/terminology/#terminology", 
            "text": "This pages contains terminology used around elastic, docker, orchestration, and deployment to get a common understanding used among teams, partners and customers.", 
            "title": "Terminology"
        }, 
        {
            "location": "/terminology/#d", 
            "text": "", 
            "title": "D"
        }, 
        {
            "location": "/terminology/#docker-swarm-docker-native-clustering-system", 
            "text": "", 
            "title": "Docker swarm - Docker native clustering system"
        }, 
        {
            "location": "/terminology/#e", 
            "text": "", 
            "title": "E"
        }, 
        {
            "location": "/terminology/#elasticsearch-open-source-distributed-restful-search-engine", 
            "text": "", 
            "title": "Elasticsearch - Open Source, Distributed, RESTful Search Engine"
        }, 
        {
            "location": "/terminology/#enigma-framework-used-for-communicating-with-qlik-engine", 
            "text": "", 
            "title": "Enigma - Framework used for communicating with qlik engine."
        }, 
        {
            "location": "/terminology/#h", 
            "text": "", 
            "title": "H"
        }, 
        {
            "location": "/terminology/#halyard-library-used-for-loading-data-into-qlik-engine", 
            "text": "", 
            "title": "Halyard - Library used for loading data into qlik engine."
        }, 
        {
            "location": "/terminology/#k", 
            "text": "", 
            "title": "K"
        }, 
        {
            "location": "/terminology/#kibana-browser-based-analytics-search-dashboard-for-elasticsearch", 
            "text": "", 
            "title": "Kibana - Browser based analytics search dashboard for Elasticsearch"
        }, 
        {
            "location": "/terminology/#kubernetes-automated-container-deployment-scaling-and-management", 
            "text": "", 
            "title": "Kubernetes - Automated container deployment, scaling, and management"
        }, 
        {
            "location": "/terminology/#l", 
            "text": "", 
            "title": "L"
        }, 
        {
            "location": "/terminology/#logstash-processes-and-transforms-data-and-sends-it-to-eg-elasticsearch", 
            "text": "", 
            "title": "Logstash - Processes and transforms data, and sends it to e.g. Elasticsearch."
        }, 
        {
            "location": "/terminology/#m", 
            "text": "", 
            "title": "M"
        }, 
        {
            "location": "/terminology/#marathon-a-container-orchestration-platform-for-mesos", 
            "text": "", 
            "title": "Marathon - A container orchestration platform for Mesos."
        }, 
        {
            "location": "/terminology/#mira-a-qix-engine-strategy-service-for-qlik-elastic", 
            "text": "", 
            "title": "Mira - A QIX Engine strategy service for Qlik Elastic."
        }, 
        {
            "location": "/terminology/#n", 
            "text": "", 
            "title": "N"
        }, 
        {
            "location": "/terminology/#nginx-http-and-reverse-proxy-server-a-mail-proxy-server-and-a-generic-tcpudp-proxy-server", 
            "text": "", 
            "title": "Nginx - HTTP and reverse proxy server, a mail proxy server, and a generic TCP/UDP proxy server"
        }, 
        {
            "location": "/terminology/#t", 
            "text": "", 
            "title": "T"
        }, 
        {
            "location": "/terminology/#traefik-http-reverse-proxy-and-load-balancer", 
            "text": "", 
            "title": "Traefik - HTTP reverse proxy and load balancer."
        }, 
        {
            "location": "/support-and-services/licensing/", 
            "text": "", 
            "title": "Licensing"
        }, 
        {
            "location": "/support-and-services/sales/", 
            "text": "Suggested", 
            "title": "Sales"
        }, 
        {
            "location": "/support-and-services/tech-support/", 
            "text": "Suggested", 
            "title": "Technical Support"
        }, 
        {
            "location": "/support-and-services/training/", 
            "text": "Suggested", 
            "title": "Training"
        }, 
        {
            "location": "/pricing/", 
            "text": "Pricing\n\n\nSuggested", 
            "title": "Pricing"
        }, 
        {
            "location": "/pricing/#pricing", 
            "text": "Suggested", 
            "title": "Pricing"
        }
    ]
}